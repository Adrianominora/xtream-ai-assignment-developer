{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Libraries and Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import MyPipe as mp\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import xgboost\n",
    "import optuna\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function and class definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class xgbReg():\n",
    "    def __init__(self, enable_categorical=True, random_state=42):\n",
    "        self.xgb = xgboost.XGBRegressor(enable_categorical=enable_categorical, random_state=random_state)\n",
    "    \n",
    "    def fit(self, x, y):\n",
    "        self.xgb.fit(x, y)\n",
    "    \n",
    "    def predict(self, x):\n",
    "        return self.xgb.predict(x)\n",
    "    \n",
    "class optParam:\n",
    "    def __init__(self, objective, enable_categorical=True, random_state=42, direction='minimize', study_name='my_study', n_trials=100):\n",
    "        self.study = optuna.create_study(direction=direction, study_name=study_name)\n",
    "        self.objective = objective\n",
    "        self.n_trials = n_trials\n",
    "        self.xgb_opt = None\n",
    "        self.enable_categorical = enable_categorical\n",
    "        self.random_state = random_state\n",
    "    \n",
    "    def fit(self, x, y):\n",
    "        self.study.optimize(self.objective, n_trials=self.n_trials)\n",
    "        self.xgb_opt = xgboost.XGBRegressor(**self.study.best_params, enable_categorical=self.enable_categorical, random_state=self.random_state)\n",
    "        self.xgb_opt.fit(x, y)\n",
    "\n",
    "    def predict(self, x):\n",
    "        return self.xgb_opt.predict(x)\n",
    "\n",
    "\n",
    "def preprocess_data(raw_data: pd.DataFrame):\n",
    "    raw_data = raw_data.loc[(raw_data.x * raw_data.y * raw_data.z != 0) & (raw_data.price > 0)] # Clean zero dimensions and negative prices\n",
    "    processed_data = raw_data.copy()\n",
    "    processed_data['cut'] = pd.Categorical(processed_data['cut'], categories=['Fair', 'Good', 'Very Good', 'Ideal', 'Premium'], ordered=True)\n",
    "    processed_data['color'] = pd.Categorical(processed_data['color'], categories=['D', 'E', 'F', 'G', 'H', 'I', 'J'], ordered=True)\n",
    "    processed_data['clarity'] = pd.Categorical(processed_data['clarity'], categories=['IF', 'VVS1', 'VVS2', 'VS1', 'VS2', 'SI1', 'SI2', 'I1'], ordered=True)\n",
    "    return processed_data\n",
    "\n",
    "def split_data(data: pd.DataFrame, test_size=0.2, random_state=42, apply_ylog = False):\n",
    "    x = data.drop(columns='price')\n",
    "    y = data['price']\n",
    "    x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=test_size, random_state=random_state)\n",
    "    if (apply_ylog):\n",
    "        y_train = np.log(y_train)\n",
    "        y_test = np.log(y_test)\n",
    "    return x_train, x_test, y_train, y_test\n",
    "\n",
    "\n",
    "    \n",
    "def plot_gof(y_true: pd.Series, y_pred: pd.Series):\n",
    "    plt.plot(y_true, y_pred, '.')\n",
    "    plt.plot(y_true, y_true, linewidth=3, c='black')\n",
    "    plt.xlabel('Actual')\n",
    "    plt.ylabel('Predicted')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "C2.1 Example on how to use the class MyPipe with the whole dataset (no data acquisition) and the custom trasformer xgbReg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diamonds = pd.read_csv(\"https://raw.githubusercontent.com/xtreamsrl/xtream-ai-assignment-engineer/main/datasets/diamonds/diamonds.csv\")\n",
    "\n",
    "data = preprocess_data(diamonds)\n",
    "x_train, x_test, y_train, y_test = split_data(data)\n",
    "\n",
    "my_steps = [('regression', xgbReg())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_pipeline = mp.MyPipe(steps=my_steps)\n",
    "my_pipeline.define_data(data)\n",
    "my_pipeline.fit(x_train,y_train)\n",
    "\n",
    "pred = my_pipeline.predict(x_test)\n",
    "performance = my_pipeline.evaluate_performance(y_test,pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_gof(y_test, pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "C2.2 Example on how to include the optuna improvement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define the objective function\n",
    "def objective(trial: optuna.trial.Trial) -> float:\n",
    "    # Define hyperparameters to tune\n",
    "    param = {\n",
    "        'lambda': trial.suggest_float('lambda', 1e-8, 1.0, log=True),\n",
    "        'alpha': trial.suggest_float('alpha', 1e-8, 1.0, log=True),\n",
    "        'colsample_bytree': trial.suggest_categorical('colsample_bytree', [0.3, 0.4, 0.5, 0.7]),\n",
    "        'subsample': trial.suggest_categorical('subsample', [0.5, 0.6, 0.7, 0.8, 0.9, 1.0]),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 1e-8, 1.0, log=True),\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 100, 1000),\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 9),\n",
    "        'random_state': 42,\n",
    "        'min_child_weight': trial.suggest_int('min_child_weight', 1, 10),\n",
    "        'enable_categorical': True\n",
    "    }\n",
    "\n",
    "    # Split the training data into training and validation sets\n",
    "    x_train, x_val, y_train, y_val = train_test_split(x_train_opt, y_train_opt, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Train the model\n",
    "    model = xgboost.XGBRegressor(**param)\n",
    "    model.fit(x_train, y_train)\n",
    "\n",
    "    # Make predictions\n",
    "    preds = model.predict(x_val)\n",
    "\n",
    "    # Calculate MAE\n",
    "    mae = mean_absolute_error(y_val, preds)\n",
    "\n",
    "    return mae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_opt, x_test_opt, y_train_opt, y_test_opt = split_data(data)\n",
    "\n",
    "my_steps = [('parameters optimization', optParam(objective, n_trials=100))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_pipeline = mp.MyPipe(steps=my_steps)\n",
    "my_pipeline.define_data(data)\n",
    "my_pipeline.fit(x_train_opt,y_train_opt)\n",
    "\n",
    "pred_opt = my_pipeline.predict(x_test_opt)\n",
    "performance = my_pipeline.evaluate_performance(y_test_opt,pred_opt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "C2.3 Simulate data acquisition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "half = int(len(data.index)/2)\n",
    "data0 = data[:half] # Suppose we know half of the data at the beginning of the procedure\n",
    "batch_size = 300 #Number of new diamonds in each batch of new data\n",
    "data_new = [] # List of the data coming in batch at every update\n",
    "i = 0\n",
    "while (half+batch_size*(i+1)<len(data.index)):\n",
    "    data_new.append(data[half+batch_size*i:half+batch_size*(i+1)])\n",
    "    i +=1\n",
    "data_new.append(data[half+batch_size*i:])\n",
    "\n",
    "# Define the first model with half of the data\n",
    "x_train, x_test, y_train, y_test = split_data(data0, apply_ylog=False)\n",
    "\n",
    "my_steps = [('parameters optimization', optParam(objective, n_trials=20))]\n",
    "\n",
    "my_pipeline = mp.MyPipe(steps=my_steps)\n",
    "my_pipeline.define_data(data0)\n",
    "my_pipeline.fit(x_train,y_train)\n",
    "\n",
    "pred = my_pipeline.predict(x_test)\n",
    "performance = my_pipeline.evaluate_performance(y_test, pred)\n",
    "my_pipeline.dump('../data/models_history/xgb_opt_model/xgb_opt_0.pkl') # save the pipeline to file\n",
    "\n",
    "for n, current_data in enumerate(data_new):\n",
    "    print(f'Batch {n+1} of {len(data_new)}')\n",
    "    my_pipeline.augment_data(current_data)\n",
    "    x_train, x_test, y_train, y_test = split_data(my_pipeline.data, apply_ylog=False)\n",
    "    my_pipeline.fit(x_train, y_train)\n",
    "    pred = my_pipeline.predict(x_test)\n",
    "    performance = my_pipeline.evaluate_performance(y_test, pred)    \n",
    "    my_pipeline.dump(f'../data/models_history/xgb_opt_model/xgb_opt_{n+1}.pkl') # save the pipeline to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_pipeline.plot_history(trendline=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
